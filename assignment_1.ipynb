{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tima0476/hw1-csci5922/blob/master/assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yI6QRw_VVhOI"
      },
      "source": [
        "# <center> Neural Networks and Deep Learning (CSCI 5922)</center>\n",
        "# <center> Spring 2020 </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oyyZSLCIVhOK"
      },
      "source": [
        "**Name:** Timothy Mason"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B7u4l5SIVhOL"
      },
      "source": [
        "## Goal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O7mbNY0lVhOL"
      },
      "source": [
        "The goal of this assignment is to introduce neural networks in terms of ideas you are already familiar with:  linear regression and classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7kNKcDdGDO4z"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w-ZVWvA4VhON"
      },
      "source": [
        "You are given a dataset with 2 input variables ($x_1$, $x_2$) and an output variable ($y$)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qluUEgHYVhOO",
        "outputId": "9b01e1d7-a9d1-4602-95b3-9955572d5fba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from matplotlib import pyplot\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    # Load data from gDrive\n",
        "    data = np.loadtxt(os.path.join('/content/drive/My Drive/spring2020/hw1-csci5922/data', 'assign1_data.txt'),  delimiter=',')\n",
        "except:\n",
        "    # Load data - relative path from notebook\n",
        "    data = np.loadtxt(os.path.join('data', 'assign1_data.txt'),  delimiter=',')\n",
        "    \n",
        "X = data[:,:2]\n",
        "y = data[:, 2]\n",
        "z = data[:, 3]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZiLhj-CvVhOQ"
      },
      "source": [
        "## Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1zNUNcC0VhOR"
      },
      "source": [
        "Write a program to find the exact least squares solution to $y = w_1 x_1 + w_2 x_2 + b$ for the above dataset, using the normal equation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "txzccR6-VhOS"
      },
      "source": [
        "Complete the following function below and use it to answer questions (A) and (B). \n",
        "\n",
        "**Note:** Please do not change the interface of the given function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gFAcJL6VVhOS",
        "colab": {}
      },
      "source": [
        "def least_squares(X, y):\n",
        "    \"\"\"\n",
        "    Finds the Least Squares solution\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : NumPy array of features (size : no of examples X features)\n",
        "    y : Numpy array of output value 'y' (size : no of examples X 1)\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    w : solution array\n",
        "    \"\"\"\n",
        "\n",
        "    # From the text:  \"Instead of adding the bias parameter b, one can continue to use \n",
        "    # the model with only weights but augment x with an extra entry that is always set \n",
        "    # to 1. The weight corresponding to the extra 1 entry plays the role of the bias \n",
        "    # parameter.\"\n",
        "\n",
        "    X = np.c_[np.ones((X.shape[0],1)), X]   # Augment X's with extra 1's\n",
        "\n",
        "    # Calculate the normal equation.\n",
        "    w = np.linalg.inv(X.T @ X) @ X.T @ y\n",
        "\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZcO-nAfxVhOV"
      },
      "source": [
        "(A) Report the values of $w_1$, $w_2$, and $b$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5sXuI05yVhOV",
        "outputId": "ed4d1394-e5f5-4d87-b70d-bb1d60d89a7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "w = least_squares(X, y)\n",
        "print(f\"w_1 = {w[1]}\\nw_2 = {w[2]}\\n  b = {w[0]}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w_1 = -2.0442425951376353\n",
            "w_2 = 3.996860168659322\n",
            "  b = -0.9242908118675891\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S-jYwaEbVhOY"
      },
      "source": [
        "(B) What function or method did you use to find the least-squares solution?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OF8A7mYEVhOY"
      },
      "source": [
        "> The linear regression was calculated using the equation: \n",
        "> \n",
        "> $$\\mathbf{w} = \\left( \\mathbf{X}^\\top \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\top \\mathbf{y}$$\n",
        "> \n",
        "> \n",
        "> The vector $\\mathbf{w}$ provides the weight coefficients for the prediction $\\hat{y}$:\n",
        "> \n",
        "> $$ \\hat{y} = \\mathbf{w}^\\top \\mathbf{x} = w_1 x_1 + w_2 x_2 + \\ldots + w_m x_m$$\n",
        "> \n",
        "> \n",
        "> Note that the requirement was to find the exact least squares solution to $y = w_1 x_1 + w_2 x_2 + b$.  \n",
        ">\n",
        "> To find the $b$ term, an extra column of 1's was inserted as the leftmost column of the $\\mathbf{X}$ matrix,\n",
        "> which adds a new term $x_0 = 1$.  This caused the normal \n",
        "> equation to solve for \n",
        "> \n",
        "> $$\n",
        "\\begin{split}\n",
        "    \\hat{y} = \\mathbf{w}^\\top \\mathbf{x} & =  w_0 x_0 + w_1 x_1 + w_2 x_2 \\\\\n",
        "    \\\\\n",
        "    \\text{Note that we set } x_0 = 1 \\text{, therefore: } \\hat{y} & = w_0 + w_1 x_1 + w_2 x_2 \\\\\n",
        "    \\\\\n",
        "    \\text{Let }w_0=b\\text{, and we have: } \\hat{y} & = w_1 x_1 + w_2 x_2 + b\n",
        "\\end{split}\n",
        "$$\n",
        ">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_qNGRMnLVhOZ"
      },
      "source": [
        "## Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K2C1Pj1xVhOZ"
      },
      "source": [
        "Implement linear regression of y on X via first-order optimization of the least-squares objective. Write a program that determines the coefficients {w1,w2,b}. Implement stochastic gradient descent, batch gradient descent, and mini-batch gradient descent. You will need to experiment with updating rules, step sizes (i.e. learning rates), stopping criteria, etc. Experiment to find settings that lead to solutions with the fewest number of sweeps through the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Dz9LPxNsVhOa"
      },
      "source": [
        "Complete the following functions below and use them to answer questions (A), (B) and (C). You may find the shuffle function from scikit-learn useful. \n",
        "\n",
        "Use the following hyperparameters:\n",
        "\n",
        "Learning rates = [0.001, 0.05, 0.01, 0.05, 0.1, 0.3]\n",
        "\n",
        "MaxIter = [10, 50, 100, 500, 1000, 5000, 10000, 25000, 50000]\n",
        "\n",
        "**Note:** Please do not change the interface of the given functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9rCKocs6VhOb",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "def online_epoch(X, y, w, alpha):\n",
        "    \"\"\"\n",
        "    One epoch of stochastic gradient descent (i.e. one sweep of the dataset).\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : NumPy array of features (size : no of examples X features)\n",
        "    y : Numpy array of class labels (size : no of examples X 1)\n",
        "    w : array of coefficients from the previous iteration\n",
        "    alpha : learning rate\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    a tuple containing Coefficients of the model (after updating) and the loss\n",
        "    \"\"\"\n",
        "    # Iterate through the training set, updating the model after each example is evaluated\n",
        "    N = X.shape[0]\n",
        "    for i in range(N):\n",
        "        x = X[i]\n",
        "        # make a prediction with our current set of weights (current example)\n",
        "        y_hat = x @ w\n",
        "\n",
        "        # compute the error (current example)\n",
        "        err = y[i] - y_hat\n",
        "\n",
        "        # compute the gradient (current example)\n",
        "        grad = -(x.T * err)\n",
        "\n",
        "        # update the model after learning from each example\n",
        "        w -= alpha*grad\n",
        "\n",
        "    # Return the updated weights and the full-training set loss.\n",
        "    return (w,np.average((y - X @ w) ** 2))\n",
        "\n",
        "\n",
        "def batch_update(X, y, w, alpha):\n",
        "    \"\"\"\n",
        "    One iteration of full-batch gradient descent.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : NumPy array of features (size : no of examples X features)\n",
        "    y : Numpy array of class labels (size : no of examples X 1)\n",
        "    w : array of coefficients from the previous iteration\n",
        "    alpha : Learning rate\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    a tuple containing Coefficients of the model (after updating) and the loss\n",
        "    \"\"\"\n",
        "    # make a prediction with our current set of weights (full training set)\n",
        "    y_hat = X @ w\n",
        "\n",
        "    # compute the error (full training set)\n",
        "    err = y - y_hat\n",
        "\n",
        "    # compute the gradient (full training set)\n",
        "    N = X.shape[0]\n",
        "    grad = (X.T @ err) * -2.0 / N\n",
        "\n",
        "    # Now we've evaluated the entire training set, update the model\n",
        "    w -= alpha*grad\n",
        "\n",
        "    # Return the updated weights and the loss.\n",
        "    return (w,np.average(err**2))\n",
        "\n",
        "def mini_batch_update(X, y, w, alpha, batch_size):\n",
        "    \"\"\"\n",
        "    One epoch of mini-batch SGD over the entire dataset (i.e. one sweep of the dataset).\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : NumPy array of features (size : no of examples X features)\n",
        "    y : Numpy array of class labels (size : no of examples X 1)\n",
        "    w : array of coefficients from the previous iteration\n",
        "    alpha : learining rate\n",
        "    batch_size : size of the batch for gradient update\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    a tuple containing Coefficients of the model (after updating) and the loss\n",
        "    \"\"\"\n",
        "    # Pull out a random subset of the training data.\n",
        "\n",
        "    mini_comb = shuffle(np.c_[X,y], n_samples = batch_size)     # shuffle combined X and y\n",
        "    X_sub = mini_comb[:,:3]\n",
        "    y_sub = mini_comb[:,3]\n",
        "\n",
        "    # make a prediction with our current set of weights (mini-batch training set)\n",
        "    y_hat = X_sub @ w\n",
        "\n",
        "    # compute the error (mini-batch training set)\n",
        "    err = y_sub - y_hat\n",
        "\n",
        "    # compute the gradient (full training set)\n",
        "    grad = (X_sub.T @ err) * -2.0 / batch_size\n",
        "\n",
        "    # Now we've evaluated the mini-batch, update the model\n",
        "    w -= alpha*grad\n",
        "\n",
        "    # Return the updated weights and the full-training set loss.\n",
        "    return (w,np.average((y - X @ w) ** 2))\n",
        "\n",
        "\n",
        "def least_squares_grad_desc(X, y, w, maxIter, alpha, update, *batch_size):\n",
        "    \"\"\"\n",
        "    Implements least squares with gradient descent.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : NumPy array of features (size : no of examples X features)\n",
        "    y : Numpy array of class labels (size : no of examples X 1)\n",
        "    w : array of coefficients from the previous iteration\n",
        "    maxIter : Maximum number of iterations allowed\n",
        "    alpha : Learning rate\n",
        "    update : update function to utilize (one of online, batch, mini-batch)\n",
        "    batch_size : number of examples in a batch (only useful when update = mini_batch_update)\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    a tuple containing Coefficients of the model (after updating) and the loss\n",
        "    \n",
        "    Note : *batch_size is an optional argument and only to be used when doing mini-batch Gradient Descent \n",
        "    \"\"\"\n",
        "    for i in range(maxIter):\n",
        "        if batch_size:\n",
        "            w,l = update(X, y, w, alpha, batch_size[0])\n",
        "        else:\n",
        "            w,l = update(X, y, w, alpha)\n",
        "\n",
        "    return (w,l)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zYMi7e8ZVhOd"
      },
      "source": [
        "(A) Report the values of $w_1$, $w_2$, and $b$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf9cfn14n3EF",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uJ2UpCAbVhOd",
        "colab": {}
      },
      "source": [
        "rates = [0.001, 0.05, 0.01, 0.05, 0.1, 0.3]\n",
        "MaxIter = [10, 50, 100, 500, 1000, 5000, 10000, 25000, 50000]\n",
        "\n",
        "Xaug = np.c_[np.ones((X.shape[0],1)), X]   # Augment X's with extra 1's for the bias term"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAYPBzpxZAU-",
        "colab_type": "text"
      },
      "source": [
        "### Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WWht3FM2VhOf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "00808cab-ce52-4abb-8dfa-112744f3fb1e"
      },
      "source": [
        "for alpha in rates:\n",
        "    w = np.random.uniform(0, 1, (Xaug.shape[1]))  # make a random initial guess at the weights\n",
        "    for mi in MaxIter:\n",
        "        w,l = least_squares_grad_desc(Xaug, y, w, mi, alpha, online_epoch)\n",
        "        print(f\"  Rate={alpha}, MaxIter:{mi:7,d}; Loss={l:9.6f}:  w1={w[1]:9.6f}, w2={w[2]:9.6f}, b={w[0]:8.5f}\")\n",
        "    print()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Rate=0.001, MaxIter:     10; Loss= 1.202875:  w1= 0.250671, w2= 0.758395, b=-0.11230\n",
            "  Rate=0.001, MaxIter:     50; Loss= 0.600319:  w1=-0.495588, w2= 1.603773, b=-0.44085\n",
            "  Rate=0.001, MaxIter:    100; Loss= 0.185652:  w1=-1.287984, w2= 2.756035, b=-0.65135\n",
            "  Rate=0.001, MaxIter:    500; Loss= 0.039596:  w1=-2.024097, w2= 3.949760, b=-0.90941\n",
            "  Rate=0.001, MaxIter:  1,000; Loss= 0.039409:  w1=-2.044018, w2= 3.997152, b=-0.92406\n",
            "  Rate=0.001, MaxIter:  5,000; Loss= 0.039409:  w1=-2.044024, w2= 3.997228, b=-0.92410\n",
            "  Rate=0.001, MaxIter: 10,000; Loss= 0.039409:  w1=-2.044024, w2= 3.997228, b=-0.92410\n",
            "  Rate=0.001, MaxIter: 25,000; Loss= 0.039409:  w1=-2.044024, w2= 3.997228, b=-0.92410\n",
            "  Rate=0.001, MaxIter: 50,000; Loss= 0.039409:  w1=-2.044024, w2= 3.997228, b=-0.92410\n",
            "\n",
            "  Rate=0.05, MaxIter:     10; Loss= 0.041826:  w1=-1.959919, w2= 3.895939, b=-0.87845\n",
            "  Rate=0.05, MaxIter:     50; Loss= 0.040411:  w1=-2.028321, w2= 4.022548, b=-0.91328\n",
            "  Rate=0.05, MaxIter:    100; Loss= 0.040411:  w1=-2.028321, w2= 4.022548, b=-0.91328\n",
            "  Rate=0.05, MaxIter:    500; Loss= 0.040411:  w1=-2.028321, w2= 4.022548, b=-0.91328\n",
            "  Rate=0.05, MaxIter:  1,000; Loss= 0.040411:  w1=-2.028321, w2= 4.022548, b=-0.91328\n",
            "  Rate=0.05, MaxIter:  5,000; Loss= 0.040411:  w1=-2.028321, w2= 4.022548, b=-0.91328\n",
            "  Rate=0.05, MaxIter: 10,000; Loss= 0.040411:  w1=-2.028321, w2= 4.022548, b=-0.91328\n",
            "  Rate=0.05, MaxIter: 25,000; Loss= 0.040411:  w1=-2.028321, w2= 4.022548, b=-0.91328\n",
            "  Rate=0.05, MaxIter: 50,000; Loss= 0.040411:  w1=-2.028321, w2= 4.022548, b=-0.91328\n",
            "\n",
            "  Rate=0.01, MaxIter:     10; Loss= 0.412500:  w1=-0.890357, w2= 1.990536, b=-0.43862\n",
            "  Rate=0.01, MaxIter:     50; Loss= 0.039894:  w1=-2.011888, w2= 3.925771, b=-0.89725\n",
            "  Rate=0.01, MaxIter:    100; Loss= 0.039446:  w1=-2.041459, w2= 4.001080, b=-0.92163\n",
            "  Rate=0.01, MaxIter:    500; Loss= 0.039446:  w1=-2.041466, w2= 4.001197, b=-0.92168\n",
            "  Rate=0.01, MaxIter:  1,000; Loss= 0.039446:  w1=-2.041466, w2= 4.001197, b=-0.92168\n",
            "  Rate=0.01, MaxIter:  5,000; Loss= 0.039446:  w1=-2.041466, w2= 4.001197, b=-0.92168\n",
            "  Rate=0.01, MaxIter: 10,000; Loss= 0.039446:  w1=-2.041466, w2= 4.001197, b=-0.92168\n",
            "  Rate=0.01, MaxIter: 25,000; Loss= 0.039446:  w1=-2.041466, w2= 4.001197, b=-0.92168\n",
            "  Rate=0.01, MaxIter: 50,000; Loss= 0.039446:  w1=-2.041466, w2= 4.001197, b=-0.92168\n",
            "\n",
            "  Rate=0.05, MaxIter:     10; Loss= 0.041818:  w1=-1.954655, w2= 3.901279, b=-0.88331\n",
            "  Rate=0.05, MaxIter:     50; Loss= 0.040411:  w1=-2.028321, w2= 4.022548, b=-0.91328\n",
            "  Rate=0.05, MaxIter:    100; Loss= 0.040411:  w1=-2.028321, w2= 4.022548, b=-0.91328\n",
            "  Rate=0.05, MaxIter:    500; Loss= 0.040411:  w1=-2.028321, w2= 4.022548, b=-0.91328\n",
            "  Rate=0.05, MaxIter:  1,000; Loss= 0.040411:  w1=-2.028321, w2= 4.022548, b=-0.91328\n",
            "  Rate=0.05, MaxIter:  5,000; Loss= 0.040411:  w1=-2.028321, w2= 4.022548, b=-0.91328\n",
            "  Rate=0.05, MaxIter: 10,000; Loss= 0.040411:  w1=-2.028321, w2= 4.022548, b=-0.91328\n",
            "  Rate=0.05, MaxIter: 25,000; Loss= 0.040411:  w1=-2.028321, w2= 4.022548, b=-0.91328\n",
            "  Rate=0.05, MaxIter: 50,000; Loss= 0.040411:  w1=-2.028321, w2= 4.022548, b=-0.91328\n",
            "\n",
            "  Rate=0.1, MaxIter:     10; Loss= 0.041242:  w1=-2.016222, w2= 4.038782, b=-0.91668\n",
            "  Rate=0.1, MaxIter:     50; Loss= 0.041248:  w1=-2.017695, w2= 4.042206, b=-0.91781\n",
            "  Rate=0.1, MaxIter:    100; Loss= 0.041248:  w1=-2.017695, w2= 4.042206, b=-0.91781\n",
            "  Rate=0.1, MaxIter:    500; Loss= 0.041248:  w1=-2.017695, w2= 4.042206, b=-0.91781\n",
            "  Rate=0.1, MaxIter:  1,000; Loss= 0.041248:  w1=-2.017695, w2= 4.042206, b=-0.91781\n",
            "  Rate=0.1, MaxIter:  5,000; Loss= 0.041248:  w1=-2.017695, w2= 4.042206, b=-0.91781\n",
            "  Rate=0.1, MaxIter: 10,000; Loss= 0.041248:  w1=-2.017695, w2= 4.042206, b=-0.91781\n",
            "  Rate=0.1, MaxIter: 25,000; Loss= 0.041248:  w1=-2.017695, w2= 4.042206, b=-0.91781\n",
            "  Rate=0.1, MaxIter: 50,000; Loss= 0.041248:  w1=-2.017695, w2= 4.042206, b=-0.91781\n",
            "\n",
            "  Rate=0.3, MaxIter:     10; Loss= 0.043659:  w1=-2.010909, w2= 4.105942, b=-0.93508\n",
            "  Rate=0.3, MaxIter:     50; Loss= 0.043659:  w1=-2.010909, w2= 4.105942, b=-0.93508\n",
            "  Rate=0.3, MaxIter:    100; Loss= 0.043659:  w1=-2.010909, w2= 4.105942, b=-0.93508\n",
            "  Rate=0.3, MaxIter:    500; Loss= 0.043659:  w1=-2.010909, w2= 4.105942, b=-0.93508\n",
            "  Rate=0.3, MaxIter:  1,000; Loss= 0.043659:  w1=-2.010909, w2= 4.105942, b=-0.93508\n",
            "  Rate=0.3, MaxIter:  5,000; Loss= 0.043659:  w1=-2.010909, w2= 4.105942, b=-0.93508\n",
            "  Rate=0.3, MaxIter: 10,000; Loss= 0.043659:  w1=-2.010909, w2= 4.105942, b=-0.93508\n",
            "  Rate=0.3, MaxIter: 25,000; Loss= 0.043659:  w1=-2.010909, w2= 4.105942, b=-0.93508\n",
            "  Rate=0.3, MaxIter: 50,000; Loss= 0.043659:  w1=-2.010909, w2= 4.105942, b=-0.93508\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m29tujWfY97n",
        "colab_type": "text"
      },
      "source": [
        "### Batch Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vf1YmHkPVhOh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "38c3aff7-2f8f-47ee-aba6-595e9e9aa1c7"
      },
      "source": [
        "for alpha in rates:\n",
        "    w = np.random.uniform(0, 1, (Xaug.shape[1]))  # make a random initial guess at the weights\n",
        "    for mi in MaxIter:\n",
        "        w,l = least_squares_grad_desc(Xaug, y, w, mi, alpha, batch_update)\n",
        "        print(f\"  Rate={alpha}, MaxIter:{mi:7,d}; Loss={l:9.6f}:  w1={w[1]:9.6f}, w2={w[2]:9.6f}, b={w[0]:8.5f}\")\n",
        "    print()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Rate=0.001, MaxIter:     10; Loss= 2.417365:  w1= 0.248359, w2= 0.809930, b= 0.72439\n",
            "  Rate=0.001, MaxIter:     50; Loss= 2.091985:  w1= 0.186034, w2= 0.781343, b= 0.61728\n",
            "  Rate=0.001, MaxIter:    100; Loss= 1.657077:  w1= 0.080455, w2= 0.743830, b= 0.44271\n",
            "  Rate=0.001, MaxIter:    500; Loss= 1.034455:  w1=-0.228687, w2= 0.775597, b= 0.02026\n",
            "  Rate=0.001, MaxIter:  1,000; Loss= 0.780377:  w1=-0.517191, w2= 1.122464, b=-0.17823\n",
            "  Rate=0.001, MaxIter:  5,000; Loss= 0.235384:  w1=-1.307424, w2= 2.497791, b=-0.51170\n",
            "  Rate=0.001, MaxIter: 10,000; Loss= 0.053323:  w1=-1.874423, w2= 3.588423, b=-0.79719\n",
            "  Rate=0.001, MaxIter: 25,000; Loss= 0.039429:  w1=-2.040456, w2= 3.980677, b=-0.91786\n",
            "  Rate=0.001, MaxIter: 50,000; Loss= 0.039409:  w1=-2.044244, w2= 3.996832, b=-0.92428\n",
            "\n",
            "  Rate=0.05, MaxIter:     10; Loss= 1.520845:  w1= 0.507338, w2= 0.311699, b=-0.04466\n",
            "  Rate=0.05, MaxIter:     50; Loss= 0.761619:  w1=-0.310291, w2= 1.292846, b=-0.37212\n",
            "  Rate=0.05, MaxIter:    100; Loss= 0.227181:  w1=-1.199891, w2= 2.596530, b=-0.61294\n",
            "  Rate=0.05, MaxIter:    500; Loss= 0.039647:  w1=-2.022346, w2= 3.943656, b=-0.90763\n",
            "  Rate=0.05, MaxIter:  1,000; Loss= 0.039409:  w1=-2.044237, w2= 3.996776, b=-0.92425\n",
            "  Rate=0.05, MaxIter:  5,000; Loss= 0.039409:  w1=-2.044243, w2= 3.996860, b=-0.92429\n",
            "  Rate=0.05, MaxIter: 10,000; Loss= 0.039409:  w1=-2.044243, w2= 3.996860, b=-0.92429\n",
            "  Rate=0.05, MaxIter: 25,000; Loss= 0.039409:  w1=-2.044243, w2= 3.996860, b=-0.92429\n",
            "  Rate=0.05, MaxIter: 50,000; Loss= 0.039409:  w1=-2.044243, w2= 3.996860, b=-0.92429\n",
            "\n",
            "  Rate=0.01, MaxIter:     10; Loss= 1.654317:  w1= 0.510831, w2= 0.811760, b= 0.12862\n",
            "  Rate=0.01, MaxIter:     50; Loss= 1.094251:  w1= 0.189823, w2= 0.858141, b=-0.24079\n",
            "  Rate=0.01, MaxIter:    100; Loss= 0.827506:  w1=-0.145799, w2= 1.203639, b=-0.39718\n",
            "  Rate=0.01, MaxIter:    500; Loss= 0.244277:  w1=-1.116741, w2= 2.548312, b=-0.62788\n",
            "  Rate=0.01, MaxIter:  1,000; Loss= 0.053420:  w1=-1.824072, w2= 3.606498, b=-0.83028\n",
            "  Rate=0.01, MaxIter:  5,000; Loss= 0.039409:  w1=-2.044125, w2= 3.996261, b=-0.92404\n",
            "  Rate=0.01, MaxIter: 10,000; Loss= 0.039409:  w1=-2.044243, w2= 3.996860, b=-0.92429\n",
            "  Rate=0.01, MaxIter: 25,000; Loss= 0.039409:  w1=-2.044243, w2= 3.996860, b=-0.92429\n",
            "  Rate=0.01, MaxIter: 50,000; Loss= 0.039409:  w1=-2.044243, w2= 3.996860, b=-0.92429\n",
            "\n",
            "  Rate=0.05, MaxIter:     10; Loss= 1.230707:  w1= 0.354631, w2= 0.691499, b=-0.25405\n",
            "  Rate=0.05, MaxIter:     50; Loss= 0.633533:  w1=-0.394908, w2= 1.589710, b=-0.48296\n",
            "  Rate=0.05, MaxIter:    100; Loss= 0.193194:  w1=-1.238692, w2= 2.752169, b=-0.67367\n",
            "  Rate=0.05, MaxIter:    500; Loss= 0.039599:  w1=-2.022868, w2= 3.949930, b=-0.91057\n",
            "  Rate=0.05, MaxIter:  1,000; Loss= 0.039409:  w1=-2.044235, w2= 3.996787, b=-0.92426\n",
            "  Rate=0.05, MaxIter:  5,000; Loss= 0.039409:  w1=-2.044243, w2= 3.996860, b=-0.92429\n",
            "  Rate=0.05, MaxIter: 10,000; Loss= 0.039409:  w1=-2.044243, w2= 3.996860, b=-0.92429\n",
            "  Rate=0.05, MaxIter: 25,000; Loss= 0.039409:  w1=-2.044243, w2= 3.996860, b=-0.92429\n",
            "  Rate=0.05, MaxIter: 50,000; Loss= 0.039409:  w1=-2.044243, w2= 3.996860, b=-0.92429\n",
            "\n",
            "  Rate=0.1, MaxIter:     10; Loss= 1.130121:  w1=-0.259768, w2= 0.546027, b= 0.01775\n",
            "  Rate=0.1, MaxIter:     50; Loss= 0.325389:  w1=-1.193018, w2= 2.199586, b=-0.41435\n",
            "  Rate=0.1, MaxIter:    100; Loss= 0.059455:  w1=-1.851155, w2= 3.510686, b=-0.76871\n",
            "  Rate=0.1, MaxIter:    500; Loss= 0.039409:  w1=-2.044203, w2= 3.996101, b=-0.92392\n",
            "  Rate=0.1, MaxIter:  1,000; Loss= 0.039409:  w1=-2.044243, w2= 3.996860, b=-0.92429\n",
            "  Rate=0.1, MaxIter:  5,000; Loss= 0.039409:  w1=-2.044243, w2= 3.996860, b=-0.92429\n",
            "  Rate=0.1, MaxIter: 10,000; Loss= 0.039409:  w1=-2.044243, w2= 3.996860, b=-0.92429\n",
            "  Rate=0.1, MaxIter: 25,000; Loss= 0.039409:  w1=-2.044243, w2= 3.996860, b=-0.92429\n",
            "  Rate=0.1, MaxIter: 50,000; Loss= 0.039409:  w1=-2.044243, w2= 3.996860, b=-0.92429\n",
            "\n",
            "  Rate=0.3, MaxIter:     10; Loss= 0.784934:  w1=-0.423518, w2= 1.298172, b=-0.32118\n",
            "  Rate=0.3, MaxIter:     50; Loss= 0.051971:  w1=-1.868206, w2= 3.631199, b=-0.82190\n",
            "  Rate=0.3, MaxIter:    100; Loss= 0.039413:  w1=-2.042492, w2= 3.989889, b=-0.92158\n",
            "  Rate=0.3, MaxIter:    500; Loss= 0.039409:  w1=-2.044243, w2= 3.996860, b=-0.92429\n",
            "  Rate=0.3, MaxIter:  1,000; Loss= 0.039409:  w1=-2.044243, w2= 3.996860, b=-0.92429\n",
            "  Rate=0.3, MaxIter:  5,000; Loss= 0.039409:  w1=-2.044243, w2= 3.996860, b=-0.92429\n",
            "  Rate=0.3, MaxIter: 10,000; Loss= 0.039409:  w1=-2.044243, w2= 3.996860, b=-0.92429\n",
            "  Rate=0.3, MaxIter: 25,000; Loss= 0.039409:  w1=-2.044243, w2= 3.996860, b=-0.92429\n",
            "  Rate=0.3, MaxIter: 50,000; Loss= 0.039409:  w1=-2.044243, w2= 3.996860, b=-0.92429\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KLLhZspY1Me",
        "colab_type": "text"
      },
      "source": [
        "### Mini-Batch Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oto8snnoYvZV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7b46a682-85bc-4bae-cbf5-c9427e7cd3e1"
      },
      "source": [
        "for alpha in rates:\n",
        "    w = np.random.uniform(0, 1, (Xaug.shape[1]))  # make a random initial guess at the weights\n",
        "    for mi in MaxIter:\n",
        "        w,l = least_squares_grad_desc(Xaug, y, w, mi, alpha, mini_batch_update, 32)\n",
        "        print(f\"  Rate={alpha}, MaxIter:{mi:7,d}; Loss={l:9.6f}:  w1={w[1]:9.6f}, w2={w[2]:9.6f}, b={w[0]:8.5f}\")\n",
        "    print()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Rate=0.001, MaxIter:     10; Loss= 2.031748:  w1= 0.057446, w2= 0.711694, b= 0.68180\n",
            "  Rate=0.001, MaxIter:     50; Loss= 1.802397:  w1= 0.005168, w2= 0.691881, b= 0.59082\n",
            "  Rate=0.001, MaxIter:    100; Loss= 1.493306:  w1=-0.085075, w2= 0.668871, b= 0.44317\n",
            "  Rate=0.001, MaxIter:    500; Loss= 1.019835:  w1=-0.354976, w2= 0.736180, b= 0.08073\n",
            "  Rate=0.001, MaxIter:  1,000; Loss= 0.773361:  w1=-0.624241, w2= 1.095971, b=-0.11146\n",
            "  Rate=0.001, MaxIter:  5,000; Loss= 0.235350:  w1=-1.356206, w2= 2.480227, b=-0.47888\n",
            "  Rate=0.001, MaxIter: 10,000; Loss= 0.053489:  w1=-1.887013, w2= 3.582091, b=-0.78837\n",
            "  Rate=0.001, MaxIter: 25,000; Loss= 0.039434:  w1=-2.041876, w2= 3.981979, b=-0.91532\n",
            "  Rate=0.001, MaxIter: 50,000; Loss= 0.039410:  w1=-2.045059, w2= 3.996267, b=-0.92453\n",
            "\n",
            "  Rate=0.05, MaxIter:     10; Loss= 1.225389:  w1= 0.088450, w2= 0.518572, b=-0.02252\n",
            "  Rate=0.05, MaxIter:     50; Loss= 0.640072:  w1=-0.574254, w2= 1.447235, b=-0.34889\n",
            "  Rate=0.05, MaxIter:    100; Loss= 0.199276:  w1=-1.310703, w2= 2.671817, b=-0.59616\n",
            "  Rate=0.05, MaxIter:    500; Loss= 0.039799:  w1=-2.021186, w2= 3.928126, b=-0.90531\n",
            "  Rate=0.05, MaxIter:  1,000; Loss= 0.039466:  w1=-2.058877, w2= 3.995384, b=-0.92333\n",
            "  Rate=0.05, MaxIter:  5,000; Loss= 0.039482:  w1=-2.033631, w2= 3.996189, b=-0.92066\n",
            "  Rate=0.05, MaxIter: 10,000; Loss= 0.039485:  w1=-2.052502, w2= 3.992948, b=-0.92706\n",
            "  Rate=0.05, MaxIter: 25,000; Loss= 0.039430:  w1=-2.038028, w2= 3.980927, b=-0.91898\n",
            "  Rate=0.05, MaxIter: 50,000; Loss= 0.039431:  w1=-2.043852, w2= 4.002552, b=-0.93163\n",
            "\n",
            "  Rate=0.01, MaxIter:     10; Loss= 2.172340:  w1= 0.794913, w2= 0.112799, b= 0.40685\n",
            "  Rate=0.01, MaxIter:     50; Loss= 1.514268:  w1= 0.456075, w2= 0.177464, b=-0.01374\n",
            "  Rate=0.01, MaxIter:    100; Loss= 1.142190:  w1= 0.060545, w2= 0.602805, b=-0.20509\n",
            "  Rate=0.01, MaxIter:    500; Loss= 0.331739:  w1=-1.002482, w2= 2.229462, b=-0.51400\n",
            "  Rate=0.01, MaxIter:  1,000; Loss= 0.059504:  w1=-1.799832, w2= 3.520868, b=-0.79527\n",
            "  Rate=0.01, MaxIter:  5,000; Loss= 0.039415:  w1=-2.052711, w2= 3.999112, b=-0.92208\n",
            "  Rate=0.01, MaxIter: 10,000; Loss= 0.039409:  w1=-2.043134, w2= 3.994820, b=-0.92431\n",
            "  Rate=0.01, MaxIter: 25,000; Loss= 0.039417:  w1=-2.045345, w2= 3.997942, b=-0.92717\n",
            "  Rate=0.01, MaxIter: 50,000; Loss= 0.039417:  w1=-2.042671, w2= 3.999740, b=-0.92910\n",
            "\n",
            "  Rate=0.05, MaxIter:     10; Loss= 1.197023:  w1= 0.515531, w2= 0.876225, b=-0.36495\n",
            "  Rate=0.05, MaxIter:     50; Loss= 0.591404:  w1=-0.324193, w2= 1.742152, b=-0.58304\n",
            "  Rate=0.05, MaxIter:    100; Loss= 0.184975:  w1=-1.207765, w2= 2.806554, b=-0.74013\n",
            "  Rate=0.05, MaxIter:    500; Loss= 0.039630:  w1=-2.027285, w2= 3.945309, b=-0.91032\n",
            "  Rate=0.05, MaxIter:  1,000; Loss= 0.039413:  w1=-2.043103, w2= 3.998004, b=-0.92328\n",
            "  Rate=0.05, MaxIter:  5,000; Loss= 0.039448:  w1=-2.049061, w2= 4.005202, b=-0.93188\n",
            "  Rate=0.05, MaxIter: 10,000; Loss= 0.039414:  w1=-2.046656, w2= 3.998775, b=-0.92213\n",
            "  Rate=0.05, MaxIter: 25,000; Loss= 0.039414:  w1=-2.045163, w2= 4.003613, b=-0.92601\n",
            "  Rate=0.05, MaxIter: 50,000; Loss= 0.039516:  w1=-2.053321, w2= 3.998558, b=-0.93110\n",
            "\n",
            "  Rate=0.1, MaxIter:     10; Loss= 1.260063:  w1= 0.256461, w2= 0.476848, b=-0.31311\n",
            "  Rate=0.1, MaxIter:     50; Loss= 0.334837:  w1=-0.936307, w2= 2.249659, b=-0.57173\n",
            "  Rate=0.1, MaxIter:    100; Loss= 0.059113:  w1=-1.780065, w2= 3.539545, b=-0.80414\n",
            "  Rate=0.1, MaxIter:    500; Loss= 0.039433:  w1=-2.031197, w2= 3.985426, b=-0.92285\n",
            "  Rate=0.1, MaxIter:  1,000; Loss= 0.039605:  w1=-2.057415, w2= 4.001396, b=-0.93409\n",
            "  Rate=0.1, MaxIter:  5,000; Loss= 0.039553:  w1=-2.039350, w2= 3.981029, b=-0.90774\n",
            "  Rate=0.1, MaxIter: 10,000; Loss= 0.039620:  w1=-2.055457, w2= 3.985480, b=-0.92758\n",
            "  Rate=0.1, MaxIter: 25,000; Loss= 0.039491:  w1=-2.043545, w2= 3.999020, b=-0.91662\n",
            "  Rate=0.1, MaxIter: 50,000; Loss= 0.039594:  w1=-2.025741, w2= 3.987098, b=-0.91545\n",
            "\n",
            "  Rate=0.3, MaxIter:     10; Loss= 0.749810:  w1=-0.450836, w2= 1.223311, b=-0.27857\n",
            "  Rate=0.3, MaxIter:     50; Loss= 0.051753:  w1=-1.928707, w2= 3.618582, b=-0.82712\n",
            "  Rate=0.3, MaxIter:    100; Loss= 0.041985:  w1=-2.078694, w2= 3.984093, b=-0.95246\n",
            "  Rate=0.3, MaxIter:    500; Loss= 0.040192:  w1=-2.064538, w2= 4.019132, b=-0.95275\n",
            "  Rate=0.3, MaxIter:  1,000; Loss= 0.039427:  w1=-2.050414, w2= 3.997901, b=-0.91817\n",
            "  Rate=0.3, MaxIter:  5,000; Loss= 0.039824:  w1=-2.067394, w2= 4.008311, b=-0.90031\n",
            "  Rate=0.3, MaxIter: 10,000; Loss= 0.039577:  w1=-2.056832, w2= 3.975343, b=-0.91906\n",
            "  Rate=0.3, MaxIter: 25,000; Loss= 0.039899:  w1=-1.986611, w2= 3.985455, b=-0.95950\n",
            "  Rate=0.3, MaxIter: 50,000; Loss= 0.039451:  w1=-2.041863, w2= 4.000201, b=-0.92053\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mMNxMJJdVhOj"
      },
      "source": [
        "(B) What settings worked well for you:  online vs. batch vs. minibatch? What step size? How did you decide to terminate?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PyFSefo3VhOk",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bXxAjKTtVhOm"
      },
      "source": [
        "(C) Make a graph of error on the entire data set as a function of epoch. An epoch is a complete sweep through all the data (which is one iteration for full-batch gradient descent)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j3rvqIrYVhOn",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u2WH5OVEVhOp"
      },
      "source": [
        "## Part 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "POTbQx4cVhOp"
      },
      "source": [
        "The data set from a regression problem can be converted into a classification problem simply by using the sign of (+ or -) as representing one of two classes. In the data set used in Part 1 and 2, you'll see the variable z that represents this binary (0 or 1) class.\n",
        "\n",
        "Use the perceptron learning rule to solve for the coefficients {$w_1$, $w_2$, $b$} of this classification problem.   \n",
        "\n",
        "Two warnings: First, your solution to Part 3 should require only a few lines of code changed from the code you wrote for Part 2. Second, the Perceptron algorithm will not converge if there is no exact solution to the training data. It will jitter among coefficients that all yield roughly equally good solutions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Qs7gbsbjVhOq"
      },
      "source": [
        "Complete the following functions below and use them to answer questions (A) and (B). \n",
        "\n",
        "**Note:** Please do not change the interface of the given functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "anej2h2pVhOr",
        "colab": {}
      },
      "source": [
        "def perceptron_update(X, y, w):\n",
        "    \"\"\"\n",
        "    One epoch of Perceptron updates (full sweep of the dataset).\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : NumPy array of features (size : no of examples X features)\n",
        "    y : Numpy array of class labels (size : no of examples X 1)\n",
        "    w : array of coefficients from the previous iteration\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    w : Coefficients of the classifier (after updating)\n",
        "    incorrect : Incorrectly classified examples\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "def perceptron(X, y, maxIter, alpha):\n",
        "    \"\"\"\n",
        "    Implements the Perceptron algorithm.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : NumPy array of features (size : no of examples X features)\n",
        "    y : Numpy array of class labels (size : no of examples X 1)\n",
        "    maxIter : The maximum number of iterations allowed \n",
        "    alpha : Learning Rate\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    w : Coefficients of the classifier\n",
        "    incorrect : Incorrectly classified examples on termination\n",
        "    \"\"\"\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CiM8viPnVhOt"
      },
      "source": [
        "(A) Report the values of coefficients $w_1$, $w_2$, and $b$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jl1YoCt-VhOt",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v9MObKUYVhOv"
      },
      "source": [
        "(B) Make a graph of the accuracy (% correct classification) on the training set as a function of epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x8yAplB5VhOw",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AfXBONZZVhOy"
      },
      "source": [
        "## Part 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zO2ij9gIVhOy"
      },
      "source": [
        "In machine learning, we really want to train a model based on some data and then expect the model to do well on \"out of sample\" data. Try this with the code you wrote for Part 3:  Train the model on the first {5, 10, 25, 50, 75} examples in the data set and test the model on the final 25 examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F66-PYWKVhOz"
      },
      "source": [
        "Complete the following function below and use it to answer (A). \n",
        "\n",
        "**Note:** Please do not change the interface of the given function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3wseIfmUVhOz",
        "colab": {}
      },
      "source": [
        "def classify(X, y, w):\n",
        "    \"\"\"\n",
        "    Use this function to classify examples in the test set\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : Test set features\n",
        "    y : Test set labels\n",
        "    w : Perceptron coefficients\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    correct : number of correctly classified examples\n",
        "    \"\"\"\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I3D_lEO8VhO3"
      },
      "source": [
        "How does performance on the test set vary with the amount of training data? Make a bar graph showing performance for each of the different training set sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0GrkjpYDVhO4",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}